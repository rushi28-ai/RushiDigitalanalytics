{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e11d4b-aa5e-4053-9851-298a90981cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Disk Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0204db-0a7a-47f8-a739-29a7ccdc3f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Optimize performance with caching on Databricks](https://docs.databricks.com/aws/en/optimizations/disk-cache)\n",
    "\n",
    "Databricks uses **disk caching** to **accelerate data reads by creating copies of remote Parquet data files in nodes' local storage** using a fast intermediate data format. \n",
    "- The data is cached automatically whenever a file has to be fetched from a remote location. \n",
    "- Successive reads of the same data are then performed locally, which results in significantly improved reading speed. \n",
    "- The cache works for all Parquet data files (including Delta Lake tables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4313abfc-b738-4bd2-8378-d81a1c96ed7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Disk cache consistency**\n",
    "\n",
    "The disk cache automatically detects when data files are created, deleted, modified, or overwritten and updates its content accordingly. You can write, modify, and delete table data with no need to explicitly invalidate cached data. Any stale entries are automatically invalidated and evicted from the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212e7e38-55cf-4baa-aee1-116bed7ebded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Disabling the cache does not result in dropping the data that is already in the local storage. Instead, it prevents queries from adding new data to the cache and reading data from the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7697fb-7f2d-4e36-b68b-8cfba4849b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"dbfs:/FileStore/Baby_Names.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73e1a06-53a1-4090-ae1c-d7de0d096590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = \"year INT, name STRING, county STRING, sex STRING, count INT\"\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(file_path)\n",
    "df.write.saveAsTable(\"baby_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b97e62-912a-4642-bd36-4c46f6cc136e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT * FROM baby_names\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccd34bb-bfb9-4301-ab2f-a1288cb3fcb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.databricks.io.cache.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7d23dd-d2ee-432c-ab4d-c6578fe1bb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e396c2-92ff-4154-b21e-42ac2400bfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT * FROM baby_names\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df85f10-ceaa-4581-9504-2572e40d8c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.sql(\"SELECT * FROM baby_names\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e1447c-3b5f-41f1-b00d-e9468283a748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- cache command preloads the data (of the queyy) on to the disk\n",
    "CACHE SELECT * FROM baby_names;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1dd4d4f-f151-4df7-a422-918fc399e038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark Persistance vs. Disk Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fcff27-c050-4983-aaa2-bcb2b0988679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Spark persistence** is a manual process to store a DataFrame or RDD in memory or on disk, while **Databricks disk cache** is an automatic, background process that caches remote Parquet files on local SSDs for faster reads. \n",
    "\n",
    "- Spark's methods (like `.cache()` or `.persist()`) require explicit code, whereas disk cache is enabled via configuration and is ideal for large Parquet tables, while Spark's manual methods are better for reusing intermediate DataFrame computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "273eb222-16a6-48b6-941f-38ca81f19ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature   | Spark Persistence   | Databricks Disk Cache   |\n",
    "|------------|------------|------------|\n",
    "| **Trigger**   | Manual (e.g. df.persist())   | Automatic, on first read of a file   |\n",
    "| **Data storage**  | JVM memory or disk, depending on storage level  | Local SSDs on worker nodes  |\n",
    "| **Data type**    | Any DataFrame or RDD     | Remote Parquet files    |\n",
    "| **Use case**     | Reusing intermediate computations from DataFrame/RDDs | Accelerating reads of frequently accessed Parquet tables     |\n",
    "| **Eviction**     | LRU or manual unpersist()     | LRU or when the underlying file changes     |\n",
    "| **Control**     | Fine-grained control over storage level and cache location     | Primarily enabled/disabled via configuration    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abeaedda-d978-4d19-a902-d4546658b621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6531036958352122,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Disk Cache",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
